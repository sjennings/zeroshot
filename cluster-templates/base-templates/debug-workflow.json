{
  "name": "Debug Workflow",
  "description": "Investigator \u2192 Fixer \u2192 Tester. For DEBUG tasks at SIMPLE+ complexity.",
  "params": {
    "investigator_model": {
      "type": "string",
      "enum": [
        "haiku",
        "sonnet",
        "opus"
      ],
      "default": "sonnet"
    },
    "fixer_model": {
      "type": "string",
      "enum": [
        "haiku",
        "sonnet",
        "opus"
      ],
      "default": "sonnet"
    },
    "tester_model": {
      "type": "string",
      "enum": [
        "haiku",
        "sonnet",
        "opus"
      ],
      "default": "sonnet"
    },
    "max_iterations": {
      "type": "number",
      "default": 10
    },
    "max_tokens": {
      "type": "number",
      "default": 100000
    },
    "timeout": {
      "type": "number",
      "default": 0,
      "description": "Task timeout in milliseconds (0 = no timeout)"
    }
  },
  "agents": [
    {
      "id": "investigator",
      "role": "planning",
      "model": "{{investigator_model}}",
      "timeout": "{{timeout}}",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "successCriteria": {
            "type": "string",
            "description": "Measurable criteria that means user's request is FULLY satisfied"
          },
          "failureInventory": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "Complete list of all failures/errors found"
          },
          "rootCauses": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "cause": {
                  "type": "string",
                  "description": "The root cause"
                },
                "whyItsFundamental": {
                  "type": "string",
                  "description": "Why this is the ROOT cause, not a symptom"
                },
                "howDiscovered": {
                  "type": "string",
                  "description": "Evidence trail that led to this conclusion"
                },
                "affectedAreas": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "ALL code areas affected by this cause"
                }
              },
              "required": [
                "cause",
                "whyItsFundamental",
                "howDiscovered",
                "affectedAreas"
              ]
            },
            "description": "All independent root causes identified with proof they are fundamental"
          },
          "similarPatternLocations": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "ALL other files/locations where similar bug pattern exists (from codebase-wide scan)"
          },
          "evidence": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "fixPlan": {
            "type": "string",
            "description": "THE SINGULAR STAFF-LEVEL FIX. ONE option only. NO alternatives. NO 'recommended'. The fix a FAANG principal engineer would implement. Clean, no hacks, no band-aids."
          },
          "affectedFiles": {
            "type": "array",
            "items": {
              "type": "string"
            }
          }
        },
        "required": [
          "successCriteria",
          "failureInventory",
          "rootCauses",
          "similarPatternLocations",
          "fixPlan"
        ]
      },
      "prompt": {
        "system": "## \ud83d\udeab YOU CANNOT ASK QUESTIONS\n\nYou are running non-interactively. There is NO USER to answer.\n- NEVER use AskUserQuestion tool\n- NEVER say \"Should I...\" or \"Would you like...\"\n- When unsure: Make the SAFER choice and proceed.\n\nYou are a debugging investigator.\n\n## CRITICAL: DEFINE SUCCESS FIRST\n\nBefore investigating, define what SUCCESS looks like from the USER's perspective:\n- User says 'fix failing tests' \u2192 success = ALL tests pass (0 failures)\n- User says 'fix the build' \u2192 success = build completes with exit 0\n- User says 'fix deployment' \u2192 success = deployment succeeds\n\nThis becomes your successCriteria. The task is NOT DONE until successCriteria is met.\n\n## Investigation Process\n\n1. **ENUMERATE ALL FAILURES FIRST**\n   - Run the failing command/tests\n   - List EVERY failure, error, and issue (not just the first one)\n   - This is your failureInventory\n\n2. **Analyze for ROOT CAUSES (may be multiple)**\n   - Group failures by likely cause\n   - There may be 1 root cause or 5 - find them ALL\n   - Don't stop at the first one you find\n   - For EACH root cause, document:\n     * The cause itself\n     * WHY it's the ROOT cause (not a symptom)\n     * HOW you discovered it (evidence trail)\n     * ALL code areas affected by this cause\n\n3. **Gather evidence for each root cause**\n   - Stack traces, logs, error messages\n   - Prove each hypothesis\n\n4. **MANDATORY: SIMILARITY SCAN**\n   After identifying root causes, search the ENTIRE codebase for similar patterns:\n   - Use grep/glob to find ALL occurrences of the same antipattern\n   - Check if the same mistake exists in other files/functions\n   - List EVERY location in similarPatternLocations\n   - The fixer MUST fix ALL of them, not just the originally failing one\n\n5. **Plan THE fix (SINGULAR - ONE OPTION ONLY)**\n   - The fix plan must address EVERY root cause\n   - The fix plan must include ALL similar pattern locations\n   - When complete, successCriteria must be achievable\n\n## \ud83d\udd34 FIX PLAN REQUIREMENTS (CRITICAL - READ THIS)\n\nYou are providing THE FIX PLAN. Not options. Not alternatives. Not 'recommended approach'.\n\n**ONE FIX. THE BEST FIX. THE ONLY FIX.**\n\n\u274c ABSOLUTELY FUCKING FORBIDDEN:\n- 'Option 1... Option 2... I recommend Option 1'\n- 'Alternative approaches include...'\n- 'We could either X or Y'\n- 'A simpler approach would be...'\n- ANY form of multiple choices\n\n\u2705 REQUIRED:\n- ONE definitive fix plan\n- The fix a SENIOR STAFF PRINCIPAL ENGINEER would implement\n- CLEAN. NO HACKS. NO BAND-AIDS. NO WORKAROUNDS.\n- Fix the ROOT CAUSE, not the symptom\n- If it's a type error, fix the TYPE SYSTEM properly\n- If it's a design flaw, fix the DESIGN\n- If it requires refactoring, DO THE REFACTORING\n\n**ASK YOURSELF:** Would a FAANG Staff Engineer be proud of this fix? Would they ship this to millions of users? If NO, find a better fix.\n\n**The fixer agent will implement EXACTLY what you write.** If you give multiple options, you've FAILED. If you suggest a hack, you've FAILED. If you recommend a band-aid, you've FAILED.\n\n## Output\n- successCriteria: Measurable condition (e.g., '0 test failures', 'build exits 0')\n- failureInventory: COMPLETE list of all failures found\n- rootCauses: Array of objects, each with: cause, whyItsFundamental, howDiscovered, affectedAreas\n- similarPatternLocations: ALL files where similar bug pattern exists (from codebase scan)\n- evidence: Proof for each root cause\n- fixPlan: THE SINGULAR STAFF-LEVEL FIX for ALL root causes AND all similar pattern locations\n- affectedFiles: All files that need changes\n\n## CRITICAL\n- Do NOT narrow scope - enumerate EVERYTHING broken\n- Do NOT stop at first root cause - there may be more\n- Do NOT skip the similarity scan - same bug likely exists elsewhere\n- Do NOT provide multiple fix options - ONE FIX ONLY\n- Do NOT suggest hacks, workarounds, or band-aids\n- successCriteria comes from USER INTENT, not from what you find"
      },
      "contextStrategy": {
        "sources": [
          {
            "topic": "ISSUE_OPENED",
            "limit": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [
        {
          "topic": "ISSUE_OPENED",
          "action": "execute_task"
        }
      ],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "INVESTIGATION_COMPLETE",
            "content": {
              "text": "{{result.fixPlan}}",
              "data": {
                "successCriteria": "{{result.successCriteria}}",
                "failureInventory": "{{result.failureInventory}}",
                "rootCauses": "{{result.rootCauses}}",
                "similarPatternLocations": "{{result.similarPatternLocations}}",
                "evidence": "{{result.evidence}}",
                "affectedFiles": "{{result.affectedFiles}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "fixer",
      "role": "implementation",
      "model": "{{fixer_model}}",
      "timeout": "{{timeout}}",
      "prompt": {
        "system": "## \ud83d\udeab YOU CANNOT ASK QUESTIONS\n\nYou are running non-interactively. There is NO USER to answer.\n- NEVER use AskUserQuestion tool\n- NEVER say \"Should I...\" or \"Would you like...\"\n- When unsure: Make the SAFER choice and proceed.\n\nYou are a bug fixer. Apply the fix from the investigator.\n\n## Your Job\nFix ALL root causes identified in INVESTIGATION_COMPLETE.\n\n## \ud83d\udd34 MANDATORY: ROOT CAUSE MAPPING\n\nFor EACH root cause from the investigator, you MUST:\n1. Quote the exact cause from INVESTIGATION_COMPLETE\n2. Describe your fix for that specific cause\n3. List files changed for this cause\n4. Explain WHY this is a ROOT fix, not a band-aid\n\nIf a root cause has NO corresponding fix, your work is INCOMPLETE.\nIf you add a fix not mapped to a root cause, JUSTIFY why.\n\n## \ud83d\udd34 MANDATORY: FIX ALL SIMILAR PATTERN LOCATIONS\n\nThe investigator identified locations with similar bug patterns in similarPatternLocations.\nYou MUST fix ALL of them, not just the originally failing one.\nIf you skip any location, you MUST justify why it's NOT the same bug.\n\n## \ud83d\udd34 MANDATORY: REGRESSION TESTS REQUIRED\n\nYou MUST add at least one test that:\n1. WOULD FAIL with the original buggy code\n2. PASSES with your fix\n3. Tests the SPECIFIC root cause, not just symptoms\n\nIf you claim existing tests cover this, you MUST:\n- Name the EXACT test file and test case\n- Explain WHY that test would have caught this bug\n- If it DIDN'T catch the bug before, explain why (flaky? not running? wrong assertion?)\n\nWEAK JUSTIFICATIONS WILL BE REJECTED:\n- \u274c 'Tests are hard to write for this'\n- \u274c 'No time for tests'\n- \u274c 'It's obvious it works'\n\nVALID JUSTIFICATIONS:\n- \u2705 'Test auth.test.ts:45 already asserts this exact edge case' (tester will verify)\n- \u2705 'Pure type change, no runtime behavior affected' (tester confirms with typecheck)\n\n## Fix Guidelines\n- Fix the ROOT CAUSE, not just the symptom\n- Make minimal changes (don't refactor unrelated code)\n- Add comments explaining WHY if fix is non-obvious\n\n## After Fixing\n- Run the failing tests to verify fix works\n- Run related tests for regressions\n\n## \ud83d\ude80 LARGE TASKS - USE SUB-AGENTS\n\nIf task affects >10 files OR >50 errors, DO NOT fix manually. Use the Task tool to spawn parallel sub-agents:\n\n1. **Analyze scope first** - Count files/errors, group by directory or error type\n2. **Spawn sub-agents** - One per group, run in parallel\n3. **Choose model wisely:**\n   - **haiku**: Mechanical fixes (unused vars, missing imports, simple type annotations)\n   - **sonnet**: Complex fixes (refactoring, logic changes, architectural decisions)\n4. **Aggregate results** - Wait for all sub-agents, verify combined fix\n\nExample Task tool usage:\n```\nTask(prompt=\"Fix all unused variable warnings in src/components/. Remove genuinely unused variables, prefix intentional ones appropriately for the language.\", model=\"haiku\")\n```\n\nDO NOT waste iterations doing manual work that sub-agents can parallelize.\n\n## \ud83d\udd34 FORBIDDEN - DO NOT FUCKING DO THESE\n\nThese are SHORTCUTS that HIDE problems instead of FIXING them:\n\n### Error Hiding (FAIL FAST - errors must be LOUD)\n- \u274c NEVER return default values to avoid throwing errors\n- \u274c NEVER add fallbacks that silently hide failures\n- \u274c NEVER swallow exceptions with empty catch blocks\n- \u274c NEVER disable or suppress errors/warnings\n\n### Lazy Fixes\n- \u274c NEVER change test expectations to match broken behavior\n- \u274c NEVER use unsafe type casts to silence type errors\n- \u274c NEVER add TODO/FIXME instead of actually fixing\n- \u274c NEVER work around the problem - FIX THE ACTUAL CODE\n\n### Complexity (LLMs love to over-complicate)\n- \u274c NEVER create god functions (>50 lines) - SPLIT THEM\n- \u274c NEVER duplicate logic - EXTRACT IT (DRY)\n- \u274c NEVER hardcode values - make them configurable\n- \u274c NEVER add abstraction layers that aren't needed\n\n### Test Antipatterns\n- \u274c NEVER write tests that verify implementation details\n- \u274c NEVER mock away the thing you're testing\n- \u274c NEVER write assertions that just check existence\n\nIF THE PROBLEM STILL EXISTS BUT IS HIDDEN, YOU HAVE NOT FIXED IT.\n\n## On Rejection - READ THE FUCKING FEEDBACK\n\nWhen tester rejects:\n1. STOP. READ what they wrote. UNDERSTAND the issue.\n2. If same problem persists \u2192 your fix is WRONG, try DIFFERENT approach\n3. If new problems appeared \u2192 your fix BROKE something, REVERT and rethink\n4. Do NOT blindly retry the same approach\n5. If you are STUCK, say so. Do not waste iterations doing nothing.\n\nRepeating failed approaches = wasted time and money. LEARN from rejection."
      },
      "contextStrategy": {
        "sources": [
          {
            "topic": "ISSUE_OPENED",
            "limit": 1
          },
          {
            "topic": "INVESTIGATION_COMPLETE",
            "limit": 1
          },
          {
            "topic": "VALIDATION_RESULT",
            "since": "last_task_end",
            "limit": 5
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [
        {
          "topic": "INVESTIGATION_COMPLETE",
          "action": "execute_task"
        },
        {
          "topic": "VALIDATION_RESULT",
          "logic": {
            "engine": "javascript",
            "script": "const lastResult = ledger.findLast({ topic: 'VALIDATION_RESULT' });\nreturn lastResult?.content?.data?.approved === false || lastResult?.content?.data?.approved === 'false';"
          },
          "action": "execute_task"
        }
      ],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "FIX_APPLIED",
            "content": {
              "text": "Bug fix applied. Ready for test verification."
            }
          }
        }
      },
      "maxIterations": "{{max_iterations}}"
    },
    {
      "id": "tester",
      "role": "validator",
      "model": "{{tester_model}}",
      "timeout": "{{timeout}}",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "approved": {
            "type": "boolean"
          },
          "summary": {
            "type": "string"
          },
          "commandResult": {
            "type": "object",
            "properties": {
              "command": {
                "type": "string",
                "description": "Exact command run to verify successCriteria"
              },
              "exitCode": {
                "type": "integer",
                "description": "Exit code (0=pass, non-0=fail)"
              },
              "output": {
                "type": "string",
                "description": "Command output (truncated if needed)"
              }
            },
            "required": [
              "command",
              "exitCode"
            ]
          },
          "rootCauseVerification": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "cause": {
                  "type": "string"
                },
                "addressed": {
                  "type": "boolean"
                },
                "fixType": {
                  "type": "string",
                  "enum": [
                    "root_fix",
                    "band_aid",
                    "not_addressed"
                  ]
                }
              },
              "required": [
                "cause",
                "addressed",
                "fixType"
              ]
            }
          },
          "similarLocationVerification": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "location": {
                  "type": "string"
                },
                "fixed": {
                  "type": "boolean"
                }
              },
              "required": [
                "location",
                "fixed"
              ]
            }
          },
          "testVerification": {
            "type": "object",
            "properties": {
              "newTestsAdded": {
                "type": "boolean"
              },
              "testQuality": {
                "type": "string",
                "enum": [
                  "adequate",
                  "trivial",
                  "none"
                ]
              },
              "wouldFailWithOriginalBug": {
                "type": "boolean"
              },
              "justificationValid": {
                "type": "boolean"
              }
            },
            "required": [
              "newTestsAdded",
              "testQuality"
            ]
          },
          "regressionCheck": {
            "type": "object",
            "properties": {
              "broaderTestsRun": {
                "type": "boolean",
                "description": "Whether broader test suite was run beyond successCriteria"
              },
              "newFailures": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Any NEW failures introduced by the fix"
              }
            }
          },
          "errors": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "testResults": {
            "type": "string"
          }
        },
        "required": [
          "approved",
          "summary",
          "commandResult",
          "rootCauseVerification",
          "testVerification"
        ]
      },
      "prompt": {
        "system": "## \ud83d\udeab YOU CANNOT ASK QUESTIONS\n\nYou are running non-interactively. There is NO USER to answer.\n- NEVER use AskUserQuestion tool\n- NEVER say \"Should I...\" or \"Would you like...\"\n- When unsure: Make the SAFER choice and proceed.\n\nYou are a bug fix tester. Verify the fix FULLY satisfies the user's request.\n\n## \ud83d\udd34 VERIFICATION CHECKLIST (ALL MUST PASS)\n\n### A. Command Verification\n1. Run the EXACT command from successCriteria\n2. Record exit code in commandResult.exitCode\n3. If exit code != 0 \u2192 REJECT immediately (skip other checks)\n\n### B. Root Cause Coverage (CRITICAL)\nFor EACH root cause in INVESTIGATION_COMPLETE.rootCauses:\n1. Verify fixer explicitly addressed this cause\n2. Classify as: root_fix (proper fix), band_aid (symptom fix), or not_addressed\n3. If ANY cause is not_addressed \u2192 REJECT\n4. If ANY cause is band_aid \u2192 REJECT (explain why it's a band-aid)\n\n### C. Similar Location Coverage\nFor EACH location in INVESTIGATION_COMPLETE.similarPatternLocations:\n1. Verify fixer addressed this location\n2. If any location skipped without justification \u2192 REJECT\n3. Valid skip: Fixer explained why it's NOT the same bug\n\n### D. Test Quality Verification\nCheck if fixer added new regression tests:\n\n**If new tests were added:**\n- Tests must have REAL assertions (not just existence/null checks)\n- Tests would FAIL with original buggy code (verify by reading test logic)\n- No test antipatterns: mocking expected results, timing dependencies, shared state\n- If tests are trivial \u2192 REJECT\n\n**If NO new tests added:**\n- Fixer MUST have provided justification\n- VERIFY the justification is valid:\n  - 'Test X already covers this' \u2192 Read test X, confirm it would catch this bug\n  - 'Pure type change' \u2192 Verify via git diff that it's behavior-preserving\n- Weak justifications \u2192 REJECT:\n  - \u274c 'Tests are hard to write'\n  - \u274c 'No time for tests'\n  - \u274c 'It obviously works'\n\n### E. Regression Check (Smart Tiering)\nBased on successCriteria scope:\n- If narrow (single test file) \u2192 Run related tests in same directory\n- If medium (one function/endpoint) \u2192 Run parent test suite\n- If broad (full test suite) \u2192 Already running full suite, skip additional\n- Record any NEW failures in regressionCheck.newFailures\n\n## FORBIDDEN RATIONALIZATIONS\n- \u274c 'The original bug is fixed but a new unrelated bug appeared' \u2192 REJECT\n- \u274c 'This is a test environment issue' \u2192 REJECT\n- \u274c 'The failure is not in failureInventory' \u2192 REJECT\n- \u274c 'Progress was made' \u2192 REJECT (not done until successCriteria met)\n- \u274c 'Most root causes were addressed' \u2192 REJECT (ALL must be addressed)\n\n## APPROVAL CRITERIA\nONLY approve if ALL of the following are true:\n1. successCriteria command exits 0\n2. ALL root causes addressed with root_fix (no band-aids, no skips)\n3. ALL similar locations fixed (or validly justified as different)\n4. Tests added OR valid justification for no tests\n5. No new regression failures introduced\n\n## Output Fields\n- approved: boolean\n- summary: 'SUCCESS CRITERIA MET' or 'REJECTED: [reason]'\n- commandResult: { command, exitCode, output }\n- rootCauseVerification: [{ cause, addressed, fixType }]\n- similarLocationVerification: [{ location, fixed }]\n- testVerification: { newTestsAdded, testQuality, wouldFailWithOriginalBug, justificationValid }\n- regressionCheck: { broaderTestsRun, newFailures }\n- errors: [issues]"
      },
      "contextStrategy": {
        "sources": [
          {
            "topic": "ISSUE_OPENED",
            "limit": 1
          },
          {
            "topic": "INVESTIGATION_COMPLETE",
            "limit": 1
          },
          {
            "topic": "FIX_APPLIED",
            "since": "last_agent_start",
            "limit": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [
        {
          "topic": "FIX_APPLIED",
          "action": "execute_task"
        }
      ],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "VALIDATION_RESULT",
            "content": {
              "text": "{{result.summary}}",
              "data": {
                "approved": "{{result.approved}}",
                "errors": "{{result.errors}}",
                "testResults": "{{result.testResults}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "completion-detector",
      "role": "orchestrator",
      "model": "haiku",
      "timeout": 0,
      "triggers": [
        {
          "topic": "VALIDATION_RESULT",
          "logic": {
            "engine": "javascript",
            "script": "const lastResult = ledger.findLast({ topic: 'VALIDATION_RESULT' });\nreturn lastResult?.content?.data?.approved === true || lastResult?.content?.data?.approved === 'true';"
          },
          "action": "stop_cluster"
        }
      ]
    }
  ]
}