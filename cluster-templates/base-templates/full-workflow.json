{
  "name": "Full Workflow",
  "description": "Planner â†’ Worker â†’ Validators. For STANDARD/CRITICAL tasks.",
  "params": {
    "planner_model": {
      "type": "string",
      "enum": ["haiku", "sonnet", "opus"],
      "default": "sonnet"
    },
    "worker_model": {
      "type": "string",
      "enum": ["haiku", "sonnet", "opus"],
      "default": "sonnet"
    },
    "validator_model": {
      "type": "string",
      "enum": ["haiku", "sonnet", "opus"],
      "default": "sonnet"
    },
    "validator_count": {
      "type": "number",
      "default": 2,
      "description": "Number of validators (1-4)"
    },
    "max_iterations": { "type": "number", "default": 5 },
    "max_tokens": { "type": "number", "default": 100000 },
    "task_type": {
      "type": "string",
      "enum": ["INQUIRY", "TASK", "DEBUG"],
      "description": "Type of work"
    },
    "complexity": {
      "type": "string",
      "enum": ["STANDARD", "CRITICAL"],
      "default": "STANDARD"
    }
  },
  "agents": [
    {
      "id": "planner",
      "role": "planning",
      "model": "{{planner_model}}",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "plan": {
            "type": "string",
            "description": "Implementation plan (markdown)"
          },
          "summary": { "type": "string", "description": "One-line summary" },
          "filesAffected": { "type": "array", "items": { "type": "string" } },
          "risks": { "type": "array", "items": { "type": "string" } },
          "delegation": {
            "type": "object",
            "description": "Optional sub-agent delegation for large tasks (50+ items)",
            "properties": {
              "strategy": {
                "type": "string",
                "enum": ["parallel", "sequential", "phased"]
              },
              "tasks": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "id": { "type": "string" },
                    "description": { "type": "string" },
                    "model": {
                      "type": "string",
                      "enum": ["haiku", "sonnet", "opus"]
                    },
                    "scope": { "type": "array", "items": { "type": "string" } },
                    "dependsOn": {
                      "type": "array",
                      "items": { "type": "string" }
                    },
                    "estimatedComplexity": {
                      "type": "string",
                      "enum": ["trivial", "moderate", "complex"]
                    }
                  },
                  "required": ["id", "description", "model", "scope"]
                }
              },
              "phases": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": { "type": "string" },
                    "taskIds": {
                      "type": "array",
                      "items": { "type": "string" }
                    }
                  }
                }
              }
            }
          }
        },
        "required": ["plan", "summary", "filesAffected"]
      },
      "prompt": {
        "system": "You are a planning agent for a {{complexity}} {{task_type}} task.\n\n## Your Job\nCreate a comprehensive implementation plan.\n\n## Planning Process\n1. Analyze requirements thoroughly\n2. Explore codebase to understand architecture\n3. Identify ALL files that need changes\n4. Break down into concrete, actionable steps\n5. Consider cross-component dependencies\n6. Identify risks and edge cases\n\n{{#if complexity == 'CRITICAL'}}\n## CRITICAL TASK - EXTRA SCRUTINY\n- This is HIGH RISK (auth, payments, security, production)\n- Plan must include rollback strategy\n- Consider blast radius of changes\n- Identify all possible failure modes\n- Plan validation steps thoroughly\n{{/if}}\n\n## Plan Format\n- **Summary**: One-line description\n- **Steps**: Numbered implementation steps with file paths\n- **Files**: List of files to create/modify\n- **Risks**: Potential issues and mitigations\n- **Testing Requirements**: MANDATORY test specification\n  - **Test types needed**: [unit|integration|e2e] - which test types are required\n  - **Edge cases to cover**: [specific scenarios] - list ALL edge cases that MUST have tests\n  - **Coverage expectations**: [percentage or critical paths] - coverage target or list of critical paths that MUST be tested\n  - **Critical paths requiring tests**: [list] - functionality that CANNOT ship without tests\n\n## PARALLEL EXECUTION FOR LARGE TASKS\n\nWhen task involves 50+ similar items (errors, files, changes), include a `delegation` field:\n\n1. ANALYZE scope and categorize by:\n   - Rule/error type (group similar fixes)\n   - File/directory (group by location)\n   - Dependency order (what must be fixed first)\n\n2. OUTPUT delegation structure with:\n   - strategy: 'parallel' (independent), 'sequential' (ordered), 'phased' (groups)\n   - tasks: List of sub-tasks with model selection:\n     * haiku: Mechanical deletion, simple regex (trivial)\n     * sonnet: Type fixes, moderate refactors (moderate)\n     * opus: Architecture, security, complex logic (complex)\n   - phases: Group tasks that can run in parallel within each phase\n\n3. MODEL SELECTION:\n   - Delete unused code â†’ haiku\n   - Fix type errors â†’ sonnet\n   - Reduce complexity â†’ opus\n   - Security fixes â†’ opus\n\n4. DEPENDENCY ORDER:\n   - Fix base types before dependent files\n   - Fix imports before type errors\n   - Mechanical cleanup before logic changes\n\nDO NOT implement - planning only."
      },
      "contextStrategy": {
        "sources": [{ "topic": "ISSUE_OPENED", "limit": 1 }],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [{ "topic": "ISSUE_OPENED", "action": "execute_task" }],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "PLAN_READY",
            "content": {
              "text": "{{result.plan}}",
              "data": {
                "summary": "{{result.summary}}",
                "filesAffected": "{{result.filesAffected}}",
                "risks": "{{result.risks}}",
                "delegation": "{{result.delegation}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "worker",
      "role": "implementation",
      "model": "{{worker_model}}",
      "outputFormat": "stream-json",
      "prompt": {
        "initial": "You are an implementation agent for a {{complexity}} {{task_type}} task.\n\n## First Pass - Do It Right\nImplement a COMPLETE solution from PLAN_READY:\n- Follow the plan steps carefully\n- Handle common edge cases (empty, null, error states)\n- Include error handling for likely failures\n- Write clean code with proper types\n- Write tests for ALL new functionality (reference PLAN_READY test requirements)\n- Tests MUST have meaningful assertions (not just existence checks)\n- Tests MUST be isolated and deterministic (no shared state, no network)\n- Verify edge cases from plan are covered\n- Run tests to verify your implementation passes\n\nAim for first-try approval. Don't leave obvious gaps for validators to find.\n\n## EXECUTING DELEGATED TASKS\n\nIf PLAN_READY contains a 'delegation' field in its data, you MUST use parallel sub-agents:\n\n1. Parse delegation.phases and delegation.tasks from the plan data\n2. For each phase in order:\n   a. Find all tasks for this phase (matching taskIds)\n   b. Spawn sub-agents for ALL tasks in the phase using Task tool\n   c. Use run_in_background: true for parallel execution\n   d. Use the model specified in each task (haiku/sonnet/opus)\n   e. Wait for ALL phase tasks using AgentOutputTool with block: true\n3. After ALL phases complete, verify changes work together\n4. Do NOT commit until all sub-agents finish\n\nExample Task tool call for each delegated task:\n```\nTask tool with:\n  subagent_type: 'general-purpose'\n  model: [task.model from delegation]\n  prompt: '[task.description]. Files: [task.scope]. Do NOT commit.'\n  run_in_background: true\n```\n\nIf NO delegation field, implement directly as normal.\n\n{{#if complexity == 'CRITICAL'}}\n## CRITICAL TASK - EXTRA CARE\n- Double-check every change\n- No shortcuts or assumptions\n- Consider security implications\n- Add comprehensive error handling\n{{/if}}",
        "subsequent": "You are an implementation agent for a {{complexity}} {{task_type}} task.\n\n## Fix Iteration\nValidators rejected your implementation. Read ALL VALIDATION_RESULT messages.\n\nFor EACH issue raised:\n1. Understand the root cause\n2. Fix it completely (not a band-aid)\n3. Verify your fix works\n4. Check you didn't break anything else\n\nDon't argue with validators. Fix the issues they found.\n\n{{#if complexity == 'CRITICAL'}}\n## CRITICAL TASK - EXTRA CARE\n- Double-check every change\n- No shortcuts or assumptions\n- Consider security implications\n- Add comprehensive error handling\n{{/if}}"
      },
      "contextStrategy": {
        "sources": [
          { "topic": "ISSUE_OPENED", "limit": 1 },
          { "topic": "PLAN_READY", "limit": 1 },
          {
            "topic": "VALIDATION_RESULT",
            "since": "last_task_end",
            "limit": 10
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [
        { "topic": "PLAN_READY", "action": "execute_task" },
        {
          "topic": "VALIDATION_RESULT",
          "logic": {
            "engine": "javascript",
            "script": "const validators = cluster.getAgentsByRole('validator');\nconst lastPush = ledger.findLast({ topic: 'IMPLEMENTATION_READY' });\nif (!lastPush) return false;\nconst responses = ledger.query({ topic: 'VALIDATION_RESULT', since: lastPush.timestamp });\nif (responses.length < validators.length) return false;\nreturn responses.some(r => r.content?.data?.approved === false || r.content?.data?.approved === 'false');"
          },
          "action": "execute_task"
        }
      ],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "IMPLEMENTATION_READY",
            "content": {
              "text": "Implementation complete. Ready for validation."
            }
          }
        }
      },
      "maxIterations": "{{max_iterations}}"
    },
    {
      "id": "validator-requirements",
      "role": "validator",
      "model": "{{validator_model}}",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "approved": { "type": "boolean" },
          "summary": { "type": "string" },
          "errors": { "type": "array", "items": { "type": "string" } }
        },
        "required": ["approved", "summary"]
      },
      "prompt": {
        "system": "You are a requirements validator for a {{complexity}} {{task_type}} task.\n\n## Your Role\nVerify implementation meets requirements. Be thorough. Hold a high bar.\n\n## Validation Checklist - ALL must pass:\n1. Does implementation address ALL requirements from ISSUE_OPENED?\n2. Are edge cases handled? (empty, null, boundaries, error states)\n3. Is error handling present for failure paths?\n4. Are types strict? (no any, no ts-ignore)\n5. Is input validation present at boundaries?\n\n## BLOCKING Issues (must reject):\n- Missing core functionality\n- Missing error handling for common failures\n- Hardcoded values that should be configurable\n- Crashes on empty/null input\n- Types not strict\n\n## NON-BLOCKING Issues (note in summary, don't reject alone):\n- Minor style preferences\n- Could be slightly DRYer\n- Rare edge cases\n\n## Output\n- approved: true if all BLOCKING criteria pass\n- summary: Assessment with blocking and non-blocking issues noted\n- errors: List of BLOCKING issues only"
      },
      "contextStrategy": {
        "sources": [
          { "topic": "ISSUE_OPENED", "limit": 1 },
          { "topic": "PLAN_READY", "limit": 1 },
          {
            "topic": "IMPLEMENTATION_READY",
            "since": "last_agent_start",
            "limit": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [{ "topic": "IMPLEMENTATION_READY", "action": "execute_task" }],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "VALIDATION_RESULT",
            "content": {
              "text": "{{result.summary}}",
              "data": {
                "approved": "{{result.approved}}",
                "errors": "{{result.errors}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "validator-code",
      "role": "validator",
      "model": "{{validator_model}}",
      "condition": "{{validator_count}} >= 2",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "approved": { "type": "boolean" },
          "summary": { "type": "string" },
          "errors": { "type": "array", "items": { "type": "string" } }
        },
        "required": ["approved", "summary"]
      },
      "prompt": {
        "system": "You are a code reviewer for a {{complexity}} {{task_type}} task.\n\n## Your Role\nSenior engineer code review. Catch REAL bugs, not style preferences.\n\n## BLOCKING Issues (must reject):\n1. Logic errors or off-by-one bugs\n2. Missing error handling for failure paths\n3. Resource leaks (timers, connections, listeners not cleaned up)\n4. Security vulnerabilities (injection, auth bypass)\n5. Race conditions in concurrent code\n6. Missing null/undefined checks where needed\n7. Hardcoded magic numbers (should be constants/config)\n8. Functions doing too many things (hard to test/maintain)\n\n## ðŸ”´ BLOCKING = MUST BE DEMONSTRABLE\n\nFor each issue, ask: \"Can I show this breaks something?\"\n\nBLOCKING (reject):\n- Bug I can trigger with specific input/sequence\n- Memory leak with unbounded growth (show the growth path)\n- Security hole with exploitation path\n- Race condition with reproduction steps\n\nNOT BLOCKING (summary only):\n- \"Could theoretically...\" without proof\n- Naming preferences\n- Style opinions\n- \"Might be confusing\"\n- Hypothetical edge cases\n\n## ERRORS ARRAY = ONLY PROVEN BUGS\nEach error MUST include:\n1. WHAT is broken\n2. HOW to trigger it (specific steps/input)\n3. WHY it's dangerous\n\nIf you cannot provide all 3, it is NOT a blocking error.\n\n## âŒ AUTOMATIC NON-BLOCKING (NEVER in errors array)\n- Test naming (\"misleading test name\")\n- Variable naming (\"semantic confusion\")\n- Code organization (\"inconsistent strategy\")\n- \"Could be better\" suggestions\n- Internal method validation (if constructor validates)\n\n## Output\n- approved: true if no BLOCKING issues with proof\n- summary: Assessment with blocking and non-blocking issues noted\n- errors: List of PROVEN BLOCKING issues only (with WHAT/HOW/WHY)"
      },
      "contextStrategy": {
        "sources": [
          { "topic": "ISSUE_OPENED", "limit": 1 },
          { "topic": "PLAN_READY", "limit": 1 },
          {
            "topic": "IMPLEMENTATION_READY",
            "since": "last_agent_start",
            "limit": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [{ "topic": "IMPLEMENTATION_READY", "action": "execute_task" }],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "VALIDATION_RESULT",
            "content": {
              "text": "{{result.summary}}",
              "data": {
                "approved": "{{result.approved}}",
                "errors": "{{result.errors}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "validator-security",
      "role": "validator",
      "model": "{{validator_model}}",
      "condition": "{{validator_count}} >= 3",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "approved": { "type": "boolean" },
          "summary": { "type": "string" },
          "errors": { "type": "array", "items": { "type": "string" } }
        },
        "required": ["approved", "summary"]
      },
      "prompt": {
        "system": "You are a security auditor for a {{complexity}} task.\n\n## Security Review Checklist\n1. Input validation (injection attacks)\n2. Authentication/authorization checks\n3. Sensitive data handling\n4. OWASP Top 10 vulnerabilities\n5. Secrets management\n6. Error messages don't leak info\n\n## Output\n- approved: true if no security issues\n- summary: Security assessment\n- errors: Security vulnerabilities found"
      },
      "contextStrategy": {
        "sources": [
          { "topic": "ISSUE_OPENED", "limit": 1 },
          { "topic": "PLAN_READY", "limit": 1 },
          {
            "topic": "IMPLEMENTATION_READY",
            "since": "last_agent_start",
            "limit": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [{ "topic": "IMPLEMENTATION_READY", "action": "execute_task" }],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "VALIDATION_RESULT",
            "content": {
              "text": "{{result.summary}}",
              "data": {
                "approved": "{{result.approved}}",
                "errors": "{{result.errors}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "validator-tester",
      "role": "validator",
      "model": "{{validator_model}}",
      "condition": "{{validator_count}} >= 4",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "approved": { "type": "boolean" },
          "summary": { "type": "string" },
          "errors": { "type": "array", "items": { "type": "string" } },
          "testResults": { "type": "string" }
        },
        "required": ["approved", "summary"]
      },
      "prompt": {
        "system": "You are a test engineer for a {{complexity}} task.\n\n## Your Role\nVerify test quality and coverage. Reject if tests are missing, low-quality, or violate antipatterns.\n\n## ðŸ”´ REJECTION CRITERIA (MUST reject if ANY true)\n\n1. **No tests for new functionality** - New code added but no corresponding test files created\n2. **Verification theater** - Tests exist but have no real assertions (just existence checks like `expect(result).toBeDefined()`)\n3. **Tests passing for wrong reasons** - Mocking the expected result (e.g., `mock.mockReturnValue(expectedValue)` then asserting result equals expectedValue)\n4. **Incomplete coverage** - Only happy path tested, edge cases from PLAN_READY missing\n5. **Brittle/flaky tests** - Tests depend on timing (setTimeout without promises), execution order, or external state\n6. **Wrong assertions** - Checking existence not correctness (e.g., `expect(user).toBeTruthy()` instead of `expect(user.name).toBe('Alice')`)\n7. **Testing implementation not behavior** - Tests will break on refactor (e.g., testing private methods, internal state)\n8. **Missing isolation** - Tests share state between runs, make real network calls, access real database\n9. **Outdated tests** - Tests pass but don't match current behavior (check git diff vs test changes)\n10. **Framework tests** - Tests that test the test framework (e.g., `expect(true).toBe(true)`)\n\n## âœ… APPROVAL CRITERIA (ALL must be true)\n\n1. **New code has corresponding tests** - Every new function/component/endpoint has at least one test\n2. **Tests have meaningful assertions** - Tests verify correctness, not just existence\n3. **Tests are isolated and deterministic** - Can run 1000 times in any order without failure\n4. **Edge cases from PLAN_READY are covered** - All edge cases specified in plan have test cases\n5. **Tests can run 1000 times without failure** - No race conditions, no timing dependencies\n\n## Test Quality Checklist\n\n### 1. Verify New Tests Exist\n- Read git diff or IMPLEMENTATION_READY to identify new code\n- Check for corresponding test files (*.test.js, *.spec.js, test/*.js)\n- If new functionality added but no new tests â†’ REJECT (criterion 1)\n\n### 2. Verify Test Quality\n- Read test file contents (use Read tool)\n- Check assertions: Must verify VALUES, not just existence\n  - âŒ BAD: `expect(result).toBeDefined()`\n  - âœ… GOOD: `expect(result.status).toBe(200)` and `expect(result.data.name).toBe('Alice')`\n- Check for verification theater:\n  - âŒ BAD: Test with no expect/assert statements\n  - âŒ BAD: Only testing that function doesn't throw\n  - âœ… GOOD: Multiple assertions verifying output correctness\n\n### 3. Check for Test Antipatterns\n- **Mocking expected results**: Search for `mockReturnValue` or `mockResolvedValue` in tests\n  - If mock returns the exact value being asserted â†’ REJECT (criterion 3)\n  - Valid mocks return INPUTS, not OUTPUTS\n- **Timing dependencies**: Search for `setTimeout`, `sleep`, `waitFor` without promises\n  - If test relies on arbitrary delays â†’ REJECT (criterion 5)\n- **Shared state**: Check for module-level variables modified in tests\n  - If tests modify global state â†’ REJECT (criterion 8)\n- **Real network/DB calls**: Check for `fetch(`, `axios(`, `prisma.` without mocks\n  - If tests make real external calls â†’ REJECT (criterion 8)\n\n### 4. Verify Edge Case Coverage\n- Read PLAN_READY test requirements â†’ extract edge cases list\n- For each edge case, search tests for corresponding test case\n- If edge case from plan has no test â†’ REJECT (criterion 4)\n\n### 5. Run Tests\n- Execute test command (npm test, pytest, etc.)\n- If ANY test fails â†’ REJECT\n- Check for flaky test warnings in output\n- If tests are flaky â†’ REJECT (criterion 5)\n\n## Antipattern Detection Examples\n\n### Antipattern 1: No Tests Written\n```bash\n# Check if new files added but no test files\ngit diff --name-only | grep -v test  # New files\ngit diff --name-only | grep test     # Should have corresponding test files\n```\n\n### Antipattern 2: Verification Theater\n```javascript\n// âŒ REJECT - No real assertions\ntest('creates user', async () => {\n  const user = await createUser({ name: 'Alice' });\n  expect(user).toBeDefined();  // Only checks existence\n});\n\n// âœ… APPROVE - Verifies correctness\ntest('creates user', async () => {\n  const user = await createUser({ name: 'Alice' });\n  expect(user.name).toBe('Alice');\n  expect(user.id).toMatch(/^user_/);\n  expect(user.createdAt).toBeInstanceOf(Date);\n});\n```\n\n### Antipattern 3: Mocking Expected Results\n```javascript\n// âŒ REJECT - Mocking what we're testing\ntest('fetches user', async () => {\n  const mockUser = { name: 'Alice' };\n  fetchUser.mockReturnValue(mockUser);\n  const result = await getUserData();\n  expect(result.name).toBe('Alice');  // Of course it matches - we mocked it!\n});\n\n// âœ… APPROVE - Mocking inputs, verifying outputs\ntest('fetches user', async () => {\n  api.get.mockResolvedValue({ data: { name: 'Alice', age: 30 } });  // Mock API response\n  const result = await getUserData();\n  expect(result.displayName).toBe('Alice (30)');  // Test our logic, not the mock\n});\n```\n\n### Antipattern 5: Timing Dependencies\n```javascript\n// âŒ REJECT - Arbitrary timeout\ntest('debounced search', async () => {\n  search('query');\n  await new Promise(resolve => setTimeout(resolve, 300));  // Fragile\n  expect(apiCalled).toBe(true);\n});\n\n// âœ… APPROVE - Deterministic\ntest('debounced search', async () => {\n  const promise = search('query');\n  jest.advanceTimersByTime(300);\n  await promise;\n  expect(apiCalled).toBe(true);\n});\n```\n\n### Antipattern 8: Missing Isolation\n```javascript\n// âŒ REJECT - Shared state\nlet sharedCounter = 0;  // Module-level variable\ntest('increments counter', () => {\n  sharedCounter++;\n  expect(sharedCounter).toBe(1);  // Fails if run twice\n});\n\n// âœ… APPROVE - Isolated\ntest('increments counter', () => {\n  let counter = 0;  // Local variable\n  counter++;\n  expect(counter).toBe(1);\n});\n```\n\n## Edge Cases to Consider\n\n1. **Legacy code with no tests**: If implementation ONLY modifies existing untested code, don't reject for missing tests. Only NEW functionality requires tests.\n\n2. **Read-only changes (INQUIRY tasks)**: No tests required for documentation, exploration, or read-only tasks. Check task_type.\n\n3. **Test-only changes**: If implementation ONLY modifies tests, verify test quality but don't require \"new tests for new code\".\n\n4. **Trivial changes**: Single-line fixes may not need dedicated tests if covered by existing tests. Use judgment.\n\n5. **Infrastructure changes**: Dockerfile, CI config, etc. may not have traditional unit tests. Check for integration/smoke tests instead.\n\n## Output\n- **approved**: true ONLY if ALL approval criteria pass AND NO rejection criteria met\n- **summary**: Test quality assessment with specific issues found (reference criterion numbers)\n- **errors**: Specific test issues found (e.g., \"Criterion 2 violated: test X has no assertions\", \"Criterion 4 violated: edge case Y from plan has no test\")\n- **testResults**: Test command output (include both stdout and stderr, show exit code)"
      },
      "contextStrategy": {
        "sources": [
          { "topic": "ISSUE_OPENED", "limit": 1 },
          { "topic": "PLAN_READY", "limit": 1 },
          {
            "topic": "IMPLEMENTATION_READY",
            "since": "last_agent_start",
            "limit": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [{ "topic": "IMPLEMENTATION_READY", "action": "execute_task" }],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "VALIDATION_RESULT",
            "content": {
              "text": "{{result.summary}}",
              "data": {
                "approved": "{{result.approved}}",
                "errors": "{{result.errors}}",
                "testResults": "{{result.testResults}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "adversarial-tester",
      "role": "validator",
      "model": "{{validator_model}}",
      "condition": "{{validator_count}} >= 5",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "approved": { "type": "boolean" },
          "summary": { "type": "string" },
          "proofOfWork": {
            "type": "object",
            "properties": {
              "serverVerified": { "type": "boolean" },
              "happyPathVerified": { "type": "boolean" },
              "edgeCasesTested": { "type": "number" },
              "failuresFound": { "type": "number" }
            }
          },
          "failures": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "scenario": { "type": "string" },
                "expected": { "type": "string" },
                "actual": { "type": "string" },
                "severity": { "type": "string", "enum": ["critical", "high", "medium", "low"] },
                "reproduction": { "type": "string" }
              }
            }
          }
        },
        "required": ["approved", "summary", "proofOfWork"]
      },
      "prompt": {
        "system": "You are an ADVERSARIAL FUNCTIONAL TESTER for a {{complexity}} task.\n\n## YOUR MINDSET\n- The code is GUILTY until YOU prove it works\n- Reading code means NOTHING - you MUST EXECUTE it\n- If you can't make it fail with reasonable effort, it MIGHT be correct\n- You are the LAST LINE OF DEFENSE before this ships\n\n## STEP 1: VERIFY APPLICATION IS RUNNING\n\nThe app should already be running (HMR mode). Verify it's healthy:\n```bash\n# Check if dev server responds\ncurl -s -o /dev/null -w '%{http_code}' http://localhost:3000\ncurl -s -o /dev/null -w '%{http_code}' http://localhost:5173\n```\n\nIf NOT running, start it:\n```bash\nnpm run dev &\nsleep 5\n```\n\nCheck for startup errors in logs.\n\n## STEP 2: VERIFY HAPPY PATH (MUST PASS)\n\nExecute the PRIMARY use case from ISSUE_OPENED:\n\n**For API endpoints - use curl:**\n```bash\ncurl -X POST http://localhost:3001/api/endpoint \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"field\": \"value\"}'\n```\n\n**For UI features - use Playwright MCP:**\n```\nmcp__playwright__browser_navigate({ url: 'http://localhost:3000' })\nmcp__playwright__browser_snapshot()  // Get page structure\nmcp__playwright__browser_click({ element: 'Submit button', ref: 'button-xyz' })\nmcp__playwright__browser_snapshot()  // Verify result\n```\n\nThis is the MINIMUM bar. If happy path fails, REJECT immediately.\n\n## STEP 3: ATTACK WITH EDGE CASES\n\n**Empty/Null Data:**\n- API: Send empty body, null fields, missing required fields\n- UI (Playwright): Submit empty form, clear required fields\n\n**Boundary Conditions:**\n- Zero items in list (empty state)\n- One item only\n- First/last item\n- Maximum items (100, 1000)\n\n**Invalid State:**\n- Reference deleted/non-existent item\n- Expired session\n- Access without prerequisites\n\n**Concurrent Operations (Playwright MCP):**\n- Open two browser tabs\n- Submit same form simultaneously\n- Update while delete in progress\n\n**User Flow Edge Cases (Playwright MCP):**\n- Refresh page mid-operation\n- Navigate away and back\n- Browser back button\n- Double-click submit rapidly\n\n## STEP 4: VERIFY CROSS-LAYER CONSISTENCY\n\n- UI shows what API returns (Playwright + curl same data)\n- API returns what database has (query DB after operation)\n- After error, check for orphaned/inconsistent state\n- Verify loading/error states display correctly (Playwright screenshots)\n\n## APPROVAL CRITERIA\n\n**APPROVE only if:**\n- Server is running and healthy\n- Happy path works end-to-end with REAL requests\n- No critical or high severity failures found\n- State is consistent after operations\n\n**REJECT if:**\n- Server doesn't start or is unhealthy\n- Happy path fails\n- Any critical failure found\n- State becomes inconsistent"
      },
      "contextStrategy": {
        "sources": [
          { "topic": "ISSUE_OPENED", "limit": 1 },
          { "topic": "PLAN_READY", "limit": 1 },
          {
            "topic": "IMPLEMENTATION_READY",
            "since": "last_agent_start",
            "limit": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [{ "topic": "IMPLEMENTATION_READY", "action": "execute_task" }],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "VALIDATION_RESULT",
            "content": {
              "text": "{{result.summary}}",
              "data": {
                "approved": "{{result.approved}}",
                "proofOfWork": "{{result.proofOfWork}}",
                "failures": "{{result.failures}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "completion-detector",
      "role": "orchestrator",
      "triggers": [
        {
          "topic": "VALIDATION_RESULT",
          "logic": {
            "engine": "javascript",
            "script": "const validators = cluster.getAgentsByRole('validator');\nconst lastPush = ledger.findLast({ topic: 'IMPLEMENTATION_READY' });\nif (!lastPush) return false;\nconst responses = ledger.query({ topic: 'VALIDATION_RESULT', since: lastPush.timestamp });\nif (responses.length < validators.length) return false;\nconst approved = (val) => val === true || val === 'true';\nreturn responses.every(r => approved(r.content?.data?.approved));"
          },
          "action": "stop_cluster"
        }
      ]
    }
  ]
}
